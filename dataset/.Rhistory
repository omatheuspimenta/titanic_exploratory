str(titanic3)
sapply(titanic3, class)
sapply(titanic3, typeof)
print(sapply(titanic3, typeof))
print(sapply(titanic3, class))
print(sapply(titanic3, typeof))
print("Classes:")
print(sapply(titanic3, class))
print("Classes:")
print(sapply(titanic3, typeof))
describe(titanic3)
library("Hmisc")
describe(titanic3)
describe(titanic3)
## "Exploratory Analysis - Titanic"
## Regression
## @ Matheus Pimenta
## github.com/omatheuspimenta/titanic_exploratory
###########################################################
#####
# Set path
setwd("/home/matheus/Dropbox/06_doutorado/2021_01/Bioestatistica/projeto/dataset/")
library("ggplot2") #for graphics
library("dplyr") #for summary
library("plyr") #for count
library("reshape2") #for melted matrix
library("infotheo") #for information theory
library("caTools") # for split dataframe
library("ROCR") # for ROC curve
library("pROC") # for ROC curve
library("caret") # for analyze
library("mfx") # for odds
#####
# Load file
load("titanic3.RData")
#####
# Feature Selection - Information Theory
# removing some columns
titanic3$name <- NULL
titanic3$ticket <- NULL
titanic3$cabin <- NULL
titanic3$boat <- NULL
titanic3$body <- NULL
titanic3$home.dest <- NULL
titanic3$lastname <- NULL
titanic3$title <- NULL
# after this, we don't use "sibsp", "parch" and "embarked" columns.
# REMEMBER, this is ONLY A EXAMPLE! If you use this, set a threshold before!!!!!
# the column age will be converted to "categorical dummy"
#####
# Creating dummy variables for "pclass", "sex", "sibsp", "parch", "embarked", "age"
# and, "nfamily"
# using the base R
# pclass class
titanic3$class <- ifelse(titanic3$pclass=="1st",1,ifelse(titanic3$pclass=="2nd",2,ifelse(titanic3$pclass=="3rd",3,0)))
# sex dummy
titanic3$sex <- ifelse(titanic3$sex=="female",1,0)
# titanic3$parch3 <- ifelse(titanic3$parch==3,1,0)
# titanic3$parch4 <- ifelse(titanic3$parch==4,1,0)
# titanic3$parch5 <- ifelse(titanic3$parch==5,1,0)
# titanic3$parch6 <- ifelse(titanic3$parch==6,1,0)
# titanic3$parch9 <- ifelse(titanic3$parch==9,1,0)
# embarked dummy (not use)
# titanic3$Cherbourg <- ifelse(titanic3$embarked == "Cherbourg",1,0)
# titanic3$Queenstown <- ifelse(titanic3$embarked == "Queenstown",1,0)
# titanic3$Southampton <- ifelse(titanic3$embarked == "Southampton",1,0)
# age dummy
titanic3$children <- ifelse(titanic3$age<=11, 1, 0)
titanic3$teenage <- ifelse((titanic3$age>11 & titanic3$age<20), 1, 0)
titanic3$young <- ifelse((titanic3$age>20 & titanic3$age<30), 1, 0)
titanic3$adult <- ifelse((titanic3$age>30 & titanic3$age<60), 1, 0)
titanic3$old <- ifelse(titanic3$age>60, 1, 0)
# titanic3$nfamily2 <- ifelse(titanic3$nfamily == 2,1,0)
# titanic3$nfamily3 <- ifelse(titanic3$nfamily == 3,1,0)
# titanic3$nfamily4 <- ifelse(titanic3$nfamily == 4,1,0)
# titanic3$nfamily5 <- ifelse(titanic3$nfamily == 5,1,0)
# titanic3$nfamily6 <- ifelse(titanic3$nfamily == 6,1,0)
# titanic3$nfamily7 <- ifelse(titanic3$nfamily == 7,1,0)
# titanic3$nfamily8 <- ifelse(titanic3$nfamily == 8,1,0)
# titanic3$nfamily11 <- ifelse(titanic3$nfamily == 11,1,0)
#####
# Drop columns
titanic3$pclass <- NULL
titanic3$embarked <- NULL
titanic3$sibsp <- NULL
titanic3$parch <- NULL
titanic3$age <- NULL
#####
# Logistic Regression
# split dataframe
set.seed(7)
split <- sample.split(titanic3$survived, SplitRatio=0.8)
train_df <- subset(titanic3, split == "TRUE")
test_df <- subset(titanic3, split == "FALSE")
# model to train/test
lr <- glm(factor(survived)~factor(sex)+
fare+
factor(mom)+
class+
children+
teenage+
young+
adult+
old+
nfamily,
family = binomial(link="logit"),
data = train_df)
summary(lr)
# old          -0.495277   0.652925  -0.759   0.4481
# nfamily      -0.368301   0.086174  -4.274 1.92e-05 ***
#   ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# (Dispersion parameter for binomial family taken to be 1)
# Null deviance: 1392.63  on 1046  degrees of freedom
# Residual deviance:  970.32  on 1036  degrees of freedom
# AIC: 992.32
# Number of Fisher Scoring iterations: 5
# anova
anova(lr, test = "Chisq")
# class        1   61.470      1042    1007.49 4.495e-15 ***
# children     1    8.570      1041     998.92  0.003418 **
# teenage      1    0.365      1040     998.56  0.545506
# young        1    2.066      1039     996.49  0.150568
# adult        1    2.967      1038     993.52  0.084978 .
# old          1    0.473      1037     993.05  0.491666
# nfamily      1   22.733      1036     970.32 1.862e-06 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# stepwise method
step(lr, direction = 'both')
#            data = train_df)
# Coefficients:
# (Intercept)  factor(sex)1          fare  factor(mom)1         class      children       teenage
#    0.315898      2.473012      0.003163      0.879230     -0.893326      2.403567      0.830554
#       young         adult       nfamily
#    0.681305      0.528959     -0.366909
# Degrees of Freedom: 1046 Total (i.e. Null);  1037 Residual
# Null Deviance:	    1393
# Residual Deviance: 970.9 	AIC: 990.9
# Well let's remove "old" and use the model
lr <- glm(factor(survived)~factor(sex)+
fare+
factor(mom)+
class+
children+
teenage+
young+
adult+
nfamily,
family = binomial(link="logit"),
data = train_df)
# predict
predictTrain = predict(lr,type="response")
tapply(predictTrain, train_df$survived, mean) # because need the same length
#         0         1
# 0.2401660 0.6115315
# ROC Curve
# plot 1
ROCRpred <- prediction(predictTrain,train_df$survived)
ROCRperf <- performance(ROCRpred, "tpr","fpr")
plot(ROCRperf,
colorize=TRUE,
print.cutoffs.at=seq(0,1,by=0.1),
text.adj=c(-0.2,1.7),
main="ROC Curve")
# plot 2
roc1=plot.roc(train_df$survived,fitted(lr))
plot(roc1,
print.auc=TRUE,
auc.polygon=TRUE,
grid.col=c("green","red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightgreen",
print.thres=TRUE,
main = "ROC Curve")
# Confusion Matrix with threshold 0.375
test_df$pred <- as.factor(
ifelse(
predict(lr,
newdata = test_df,
type = "response") > 0.375,
1,0)
)
confusionMatrix(test_df$pred, as.factor(test_df$survived))
#             Sensitivity : 0.7901
#             Specificity : 0.7600
#          Pos Pred Value : 0.8421
#          Neg Pred Value : 0.6909
#              Prevalence : 0.6183
#          Detection Rate : 0.4885
#    Detection Prevalence : 0.5802
#       Balanced Accuracy : 0.7751
#        'Positive' Class : 0
# odds in model
logitor(factor(survived)~factor(sex)+
fare+
factor(mom)+
class+
children+
teenage+
young+
adult+
nfamily,
data = train_df)
# factor(mom)1  2.4090446  0.8746996  2.4215   0.01546 *
# class         0.4092922  0.0525979 -6.9514 3.616e-12 ***
# children     11.0625619  5.1841949  5.1290 2.913e-07 ***
# teenage       2.2945888  0.8653321  2.2024   0.02764 *
# young         1.9764548  0.6320890  2.1303   0.03314 *
# adult         1.6971654  0.5382840  1.6678   0.09536 .
# nfamily       0.6928725  0.0596033 -4.2652 1.997e-05 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# confident interval (95%)
exp(cbind(OR=coef(lr), confint(lr)))
# factor(sex)1 11.8581114 8.3171578 17.1188045
# fare          1.0031678 0.9993718  1.0074682
# factor(mom)1  2.4090446 1.2006990  5.0018492
# class         0.4092922 0.3175237  0.5259066
# children     11.0625619 4.4844576 28.2439003
# teenage       2.2945888 1.1026468  4.8493355
# young         1.9764548 1.0687351  3.7541068
# adult         1.6971654 0.9209081  3.2016328
# nfamily       0.6928725 0.5805972  0.8137376
# remove temp variables
remove(lr, roc1, ROCRperf, ROCRpred, test_df, train_df, predictTrain, split)
#####
# trying to improve accuracy
# Reload file
load("titanic3.RData")
#####
# removing some columns
titanic3$name <- NULL
titanic3$ticket <- NULL
titanic3$cabin <- NULL
titanic3$boat <- NULL
titanic3$body <- NULL
titanic3$home.dest <- NULL
titanic3$lastname <- NULL
titanic3$title <- NULL
# pclass class
titanic3$class <- ifelse(titanic3$pclass=="1st",1,ifelse(titanic3$pclass=="2nd",2,ifelse(titanic3$pclass=="3rd",3,0)))
# sex dummy
titanic3$sex <- ifelse(titanic3$sex=="female",1,0)
# Drop columns
titanic3$pclass <- NULL
titanic3$embarked <- NULL
titanic3$sibsp <- NULL
titanic3$parch <- NULL
# removing the fare outlier value and no dummy the "age" column
i <- which(titanic3$fare>500)
titanic3 <- titanic3[-i,]
remove(i)
View(titanic3)
# New logistic regression
# split dataframe
set.seed(7)
split <- sample.split(titanic3$survived, SplitRatio=0.8)
train_df <- subset(titanic3, split == "TRUE")
test_df <- subset(titanic3, split == "FALSE")
# model to train/test
lr <- glm(factor(survived)~factor(sex)+
fare+
factor(mom)+
class+
age+
nfamily,
family = binomial(link="logit"),
data = train_df)
summary(lr)
# age          -0.0399763  0.0074106  -5.394 6.87e-08 ***
# nfamily      -0.2446959  0.0695098  -3.520 0.000431 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# (Dispersion parameter for binomial family taken to be 1)
# Null deviance: 1386.8  on 1043  degrees of freedom
# Residual deviance:  966.7  on 1037  degrees of freedom
# AIC: 980.7
# Number of Fisher Scoring iterations: 5
# anova
anova(lr, test = "Chisq")
# NULL                         1043    1386.84
# factor(sex)  1  288.174      1042    1098.67 < 2.2e-16 ***
# fare         1   26.708      1041    1071.96 2.367e-07 ***
# factor(mom)  1    3.493      1040    1068.47 0.0616401 .
# class        1   64.931      1039    1003.53 7.756e-16 ***
# age          1   22.909      1038     980.63 1.699e-06 ***
# nfamily      1   13.920      1037     966.70 0.0001907 ***
#  ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# stepwise method
step(lr, direction = 'both')
# - factor(sex)  1  1243.84 1251.84
# Call:  glm(formula = survived ~ factor(sex) + class + age + nfamily,
#            family = binomial(link = "logit"), data = train_df)
# Coefficients:
#   (Intercept)  factor(sex)1         class           age       nfamily
# 2.53775       2.61602      -1.14294      -0.03761      -0.20057
# Degrees of Freedom: 1043 Total (i.e. Null);  1039 Residual
# Null Deviance:	    1387
# Residual Deviance: 968.2 	AIC: 978.2
# Now, we remove the "mom" and "fare"
lr <- glm(survived~factor(sex)+
class+
age+
nfamily,
family = binomial(link="logit"),
data = train_df)
# predict
predictTrain = predict(lr,type="response")
tapply(predictTrain, train_df$survived, mean) # because need the same length
#         0         1
# 0.2397009 0.6093539
# ROC Curve
# plot 1
ROCRpred <- prediction(predictTrain,train_df$survived)
ROCRperf <- performance(ROCRpred, "tpr","fpr")
plot(ROCRperf,
colorize=TRUE,
print.cutoffs.at=seq(0,1,by=0.1),
text.adj=c(-0.2,1.7),
main="ROC Curve")
# plot 2
roc1=plot.roc(train_df$survived,fitted(lr))
plot(roc1,
print.auc=TRUE,
auc.polygon=TRUE,
grid.col=c("green","red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightgreen",
print.thres=TRUE,
main = "ROC Curve")
# Confusion Matrix with threshold 0.375
test_df$pred <- as.factor(
ifelse(
predict(lr,
newdata = test_df,
type = "response") > 0.375,
1,0)
)
confusionMatrix(test_df$pred, as.factor(test_df$survived))
# Mcnemar's Test P-Value : 0.1182
#            Sensitivity : 0.7778
#            Specificity : 0.7677
#         Pos Pred Value : 0.8456
#         Neg Pred Value : 0.6786
#             Prevalence : 0.6207
#         Detection Rate : 0.4828
#   Detection Prevalence : 0.5709
#      Balanced Accuracy : 0.7727
#       'Positive' Class : 0
logitor(survived~factor(sex)+
class+
age+
nfamily,
data = train_df)
#           data = train_df)
# Odds Ratio:
#                 OddsRatio  Std. Err.        z     P>|z|
#   factor(sex)1 13.6812152  2.4029737  14.8942 < 2.2e-16 ***
#   class         0.3188810  0.0358203 -10.1747 < 2.2e-16 ***
#   age           0.9630912  0.0068152  -5.3144  1.07e-07 ***
#   nfamily       0.8182609  0.0466694  -3.5167  0.000437 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# confident interval (95%)
exp(cbind(OR=coef(lr), confint(lr)))
#                      OR     2.5 %     97.5 %
# (Intercept)  12.6511989 5.5872517 29.3808183
# factor(sex)1 13.6812152 9.7568552 19.4363906
# class         0.3188810 0.2548360  0.3960097
# age           0.9630912 0.9496068  0.9763484
# nfamily       0.8182609 0.7285057  0.9115658
remove(lr, roc1, ROCRperf, ROCRpred, test_df, train_df, predictTrain, split)
#####
# trying to improve accuracy
# without fare outlier value and with dummy the "age" column as "adult",
#"children" and "old"
# age dummy
titanic3$children <- ifelse(titanic3$age<=11, 1, 0)
titanic3$adult <- ifelse((titanic3$age>11 & titanic3$age<50), 1, 0)
titanic3$old <- ifelse(titanic3$age>50, 1, 0)
# Drop columns
titanic3$age <- NULL
# New logistic regression
# split dataframe
set.seed(7)
split <- sample.split(titanic3$survived, SplitRatio=0.8)
train_df <- subset(titanic3, split == "TRUE")
test_df <- subset(titanic3, split == "FALSE")
# model to train/test
lr <- glm(survived~factor(sex)+
fare+
factor(mom)+
class+
children+
adult+
old+
nfamily,
family = binomial(link="logit"),
data = train_df)
summary(lr)
# old          -0.451897   0.802447  -0.563  0.57333
# nfamily      -0.419376   0.090634  -4.627 3.71e-06 ***
#   ---
#   Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# (Dispersion parameter for binomial family taken to be 1)
# Null deviance: 1386.84  on 1043  degrees of freedom
# Residual deviance:  955.86  on 1035  degrees of freedom
# AIC: 973.86
# Number of Fisher Scoring iterations: 5
# anova
anova(lr, test = "Chisq")
# fare         1   26.708      1041    1071.96 2.367e-07 ***
# factor(mom)  1    3.493      1040    1068.47   0.06164 .
# class        1   64.931      1039    1003.53 7.756e-16 ***
# children     1   16.257      1038     987.28 5.529e-05 ***
# adult        1    3.806      1037     983.47   0.05107 .
# old          1    0.657      1036     982.81   0.41762
# nfamily      1   26.949      1035     955.86 2.089e-07 ***
#   ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# stepwise method
step(lr, direction = 'both')
#              children + old + nfamily, family = binomial(link = "logit"),
#            data = train_df)
# Coefficients:
#   (Intercept)  factor(sex)1  factor(mom)1         class      children           old       nfamily
# 1.2542        2.5707        0.7297       -0.9928        2.2004       -0.6143       -0.3862
#
# Degrees of Freedom: 1043 Total (i.e. Null);  1037 Residual
# Null Deviance:	    1387
# Residual Deviance: 957.1 	AIC: 971.1
# Now, we remove the "adult" and "fare"
lr <- glm(survived~factor(sex)+
factor(mom)+
class+
children+
old+
nfamily,
family = binomial(link="logit"),
data = train_df)
# predict
predictTrain = predict(lr,type="response")
tapply(predictTrain, train_df$survived, mean) # because need the same length
#         0         1
# 0.2363644 0.6147915
# ROC Curve
# plot 1
ROCRpred <- prediction(predictTrain,train_df$survived)
ROCRperf <- performance(ROCRpred, "tpr","fpr")
plot(ROCRperf,
colorize=TRUE,
print.cutoffs.at=seq(0,1,by=0.1),
text.adj=c(-0.2,1.7),
main="ROC Curve")
# plot 2
roc1=plot.roc(train_df$survived,fitted(lr))
plot(roc1,
print.auc=TRUE,
auc.polygon=TRUE,
grid.col=c("green","red"),
max.auc.polygon=TRUE,
auc.polygon.col="lightgreen",
print.thres=TRUE,
main = "ROC Curve")
# Confusion Matrix with threshold 0.328
test_df$pred <- as.factor(
ifelse(
predict(lr,
newdata = test_df,
type = "response") > 0.328,
1,0)
)
confusionMatrix(test_df$pred, as.factor(test_df$survived))
# Mcnemar's Test P-Value : 0.00485
#            Sensitivity : 0.7407
#            Specificity : 0.8081
#         Pos Pred Value : 0.8633
#         Neg Pred Value : 0.6557
#             Prevalence : 0.6207
#         Detection Rate : 0.4598
#   Detection Prevalence : 0.5326
#      Balanced Accuracy : 0.7744
#       'Positive' Class : 0
logitor(survived~factor(sex)+
factor(mom)+
class+
children+
old+
nfamily,
data = train_df)
#              OddsRatio Std. Err.       z     P>|z|
# factor(sex)1 13.074650  2.407140 13.9629 < 2.2e-16 ***
# factor(mom)1  2.074485  0.760266  1.9911   0.04647 *
# class         0.370526  0.038085 -9.6591 < 2.2e-16 ***
# children      9.028415  3.503536  5.6702 1.426e-08 ***
# old           0.541013  0.169742 -1.9580   0.05023 .
# nfamily       0.679631  0.057342 -4.5774 4.707e-06 ***
# ---
# Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
# confident interval (95%)
exp(cbind(OR=coef(lr), confint(lr)))
#                      OR     2.5 %     97.5 %
# (Intercept)   3.5049869 2.0741533  5.9864514
# factor(sex)1 13.0746503 9.1702111 18.8846491
# factor(mom)1  2.0744851 1.0270434  4.3355849
# class         0.3705255 0.3019903  0.4520168
# children      9.0284153 4.2911833 19.7140224
# old           0.5410132 0.2892423  0.9913836
# nfamily       0.6796310 0.5718627  0.7959590
remove(lr, roc1, ROCRperf, ROCRpred, test_df, train_df, predictTrain, split)
